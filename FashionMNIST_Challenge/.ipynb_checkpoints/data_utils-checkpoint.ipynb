{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f)\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def load_tiny_imagenet(path, dtype=np.float32):\n",
    "  \"\"\"\n",
    "  Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n",
    "  TinyImageNet-200 have the same directory structure, so this can be used\n",
    "  to load any of them.\n",
    "\n",
    "  Inputs:\n",
    "  - path: String giving path to the directory to load.\n",
    "  - dtype: numpy datatype used to load the data.\n",
    "\n",
    "  Returns: A tuple of\n",
    "  - class_names: A list where class_names[i] is a list of strings giving the\n",
    "    WordNet names for class i in the loaded dataset.\n",
    "  - X_train: (N_tr, 3, 64, 64) array of training images\n",
    "  - y_train: (N_tr,) array of training labels\n",
    "  - X_val: (N_val, 3, 64, 64) array of validation images\n",
    "  - y_val: (N_val,) array of validation labels\n",
    "  - X_test: (N_test, 3, 64, 64) array of testing images.\n",
    "  - y_test: (N_test,) array of test labels; if test labels are not available\n",
    "    (such as in student code) then y_test will be None.\n",
    "  \"\"\"\n",
    "  # First load wnids\n",
    "  with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "    wnids = [x.strip() for x in f]\n",
    "\n",
    "  # Map wnids to integer labels\n",
    "  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "\n",
    "  # Use words.txt to get names for each class\n",
    "  with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "    wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "    for wnid, words in wnid_to_words.iteritems():\n",
    "      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "  class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "\n",
    "  # Next load training data.\n",
    "  X_train = []\n",
    "  y_train = []\n",
    "  for i, wnid in enumerate(wnids):\n",
    "    if (i + 1) % 20 == 0:\n",
    "      print 'loading training data for synset %d / %d' % (i + 1, len(wnids))\n",
    "    # To figure out the filenames we need to open the boxes file\n",
    "    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
    "    with open(boxes_file, 'r') as f:\n",
    "      filenames = [x.split('\\t')[0] for x in f]\n",
    "    num_images = len(filenames)\n",
    "    \n",
    "    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n",
    "    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n",
    "    for j, img_file in enumerate(filenames):\n",
    "      img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
    "      img = imread(img_file)\n",
    "      if img.ndim == 2:\n",
    "        ## grayscale file\n",
    "        img.shape = (64, 64, 1)\n",
    "      X_train_block[j] = img.transpose(2, 0, 1)\n",
    "    X_train.append(X_train_block)\n",
    "    y_train.append(y_train_block)\n",
    "      \n",
    "  # We need to concatenate all training data\n",
    "  X_train = np.concatenate(X_train, axis=0)\n",
    "  y_train = np.concatenate(y_train, axis=0)\n",
    "  \n",
    "  # Next load validation data\n",
    "  with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "    img_files = []\n",
    "    val_wnids = []\n",
    "    for line in f:\n",
    "      img_file, wnid = line.split('\\t')[:2]\n",
    "      img_files.append(img_file)\n",
    "      val_wnids.append(wnid)\n",
    "    num_val = len(img_files)\n",
    "    y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
    "    X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n",
    "    for i, img_file in enumerate(img_files):\n",
    "      img_file = os.path.join(path, 'val', 'images', img_file)\n",
    "      img = imread(img_file)\n",
    "      if img.ndim == 2:\n",
    "        img.shape = (64, 64, 1)\n",
    "      X_val[i] = img.transpose(2, 0, 1)\n",
    "\n",
    "  # Next load test images\n",
    "  # Students won't have test labels, so we need to iterate over files in the\n",
    "  # images directory.\n",
    "  img_files = os.listdir(os.path.join(path, 'test', 'images'))\n",
    "  X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n",
    "  for i, img_file in enumerate(img_files):\n",
    "    img_file = os.path.join(path, 'test', 'images', img_file)\n",
    "    img = imread(img_file)\n",
    "    if img.ndim == 2:\n",
    "      img.shape = (64, 64, 1)\n",
    "    X_test[i] = img.transpose(2, 0, 1)\n",
    "\n",
    "  y_test = None\n",
    "  y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n",
    "  if os.path.isfile(y_test_file):\n",
    "    with open(y_test_file, 'r') as f:\n",
    "      img_file_to_wnid = {}\n",
    "      for line in f:\n",
    "        line = line.split('\\t')\n",
    "        img_file_to_wnid[line[0]] = line[1]\n",
    "    y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]\n",
    "    y_test = np.array(y_test)\n",
    "  \n",
    "  return class_names, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def load_models(models_dir):\n",
    "  \"\"\"\n",
    "  Load saved models from disk. This will attempt to unpickle all files in a\n",
    "  directory; any files that give errors on unpickling (such as README.txt) will\n",
    "  be skipped.\n",
    "\n",
    "  Inputs:\n",
    "  - models_dir: String giving the path to a directory containing model files.\n",
    "    Each model file is a pickled dictionary with a 'model' field.\n",
    "\n",
    "  Returns:\n",
    "  A dictionary mapping model file names to models.\n",
    "  \"\"\"\n",
    "  models = {}\n",
    "  for model_file in os.listdir(models_dir):\n",
    "    with open(os.path.join(models_dir, model_file), 'rb') as f:\n",
    "      try:\n",
    "        models[model_file] = pickle.load(f)['model']\n",
    "      except pickle.UnpicklingError:\n",
    "        continue\n",
    "  return models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
